#!/usr/bin/env python3
"""
Compare Multiple YOLOv11 Models
Validates multiple models and shows comparison table

Usage:
    python compare_models.py \
        --models runs/detect/*/weights/best.pt \
        --data /path/to/data.yaml \
        --split test
"""

import argparse
from pathlib import Path
from ultralytics import YOLO
import glob


def validate_single_model(model_path, data_yaml, split, imgsz=768, batch=16):
    """Validate a single model and return metrics"""
    try:
        model = YOLO(str(model_path))
        results = model.val(
            data=str(data_yaml),
            split=split,
            imgsz=imgsz,
            batch=batch,
            plots=False,
            save_json=False,
            verbose=False
        )
        
        metrics = results.results_dict
        precision = metrics.get('metrics/precision(B)', 0)
        recall = metrics.get('metrics/recall(B)', 0)
        mAP50 = metrics.get('metrics/mAP50(B)', 0)
        mAP50_95 = metrics.get('metrics/mAP50-95(B)', 0)
        
        if precision > 0 and recall > 0:
            f1 = 2 * (precision * recall) / (precision + recall)
            avg_acc = (precision + recall) / 2
        else:
            f1 = 0
            avg_acc = 0
        
        # Get experiment name from parent directory (2 levels up from best.pt)
        exp_name = model_path.parent.parent.name
        
        return {
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'mAP50': mAP50,
            'mAP50_95': mAP50_95,
            'avg_acc': avg_acc,
            'path': str(model_path),
            'name': exp_name
        }
    except Exception as e:
        print(f"âš  Error validating {model_path}: {e}")
        return None


def print_comparison_table(all_metrics):
    """Print comparison table"""
    if not all_metrics:
        print("âŒ No valid metrics found!")
        return
    
    # Sort by avg_acc descending
    all_metrics.sort(key=lambda x: x['avg_acc'], reverse=True)
    
    print("\n" + "="*80)
    print("ğŸ“Š MODEL COMPARISON (Sorted by Average Accuracy)")
    print("="*80)
    print("")
    print("â”Œâ”€" + "â”€" * 78 + "â”€â”")
    print(f"â”‚ {'MODEL':<35} {'PREC':>7} {'REC':>7} {'F1':>7} {'mAP50':>7} {'AVG_ACC':>7} {'TARGET':>6} â”‚")
    print("â”œâ”€" + "â”€" * 78 + "â”€â”¤")
    
    best_avg_acc = all_metrics[0]['avg_acc']
    best_model = all_metrics[0]
    
    for m in all_metrics:
        name = m['name'][:33]  # Truncate long names
        prec = m['precision']
        rec = m['recall']
        f1 = m['f1']
        map50 = m['mAP50']
        avg_acc = m['avg_acc']
        
        target_status = "âœ…" if avg_acc >= 0.70 else "âŒ"
        
        print(f"â”‚ {name:<35} {prec*100:>6.2f}% {rec*100:>6.2f}% {f1*100:>6.2f}% {map50*100:>6.2f}% {avg_acc*100:>6.2f}% {target_status:>6} â”‚")
    
    print("â””â”€" + "â”€" * 78 + "â”€â”˜")
    print("")
    
    # Print best model info
    print(f"ğŸ† BEST MODEL: {best_model['name']}")
    print(f"   Average Accuracy: {best_model['avg_acc']*100:.2f}%")
    print(f"   Recall: {best_model['recall']*100:.2f}%  Precision: {best_model['precision']*100:.2f}%")
    print(f"   Model: {best_model['path']}")
    print("")
    
    # Calculate gap to target
    if best_avg_acc >= 0.70:
        print("âœ… Target achieved! Ready for deployment.")
    else:
        gap = (0.70 - best_avg_acc) * 100
        print(f"ğŸ“Š Best model is {gap:.1f}% away from 70% target")
        print("")
        print("ğŸ’¡ BREAKTHROUGH ANALYSIS:")
        print(f"   ğŸ¯ Cosine LR scheduling helped! (+1.38% improvement)")
        print("")
        print("ğŸš€ Next steps to close {:.1f}% gap:".format(gap))
        if gap <= 3:
            print("   1. Push this model further with more stable fine-tuning")
            print("   2. Try YOLOv11m with cosine scheduling")
        elif gap <= 6:
            print("   1. âœ… WORKING: Cosine LR scheduling")
            print("   2. Try: Lower LR (0.0005) + longer warmup (5 epochs)")
            print("   3. Try: YOLOv11m with cosine + stable config")
            print("   4. Consider: Ensemble of top 3 models")
        else:
            print("   1. Scale up to YOLOv11m")
            print("   2. Use cosine LR scheduling")
    
    print("="*80)


def main():
    parser = argparse.ArgumentParser(
        description="Compare multiple YOLOv11 models"
    )
    
    parser.add_argument('--models', type=str, nargs='+', required=True,
                       help='Paths to model .pt files (supports wildcards)')
    parser.add_argument('--data', type=str, required=True,
                       help='Path to data.yaml file')
    parser.add_argument('--split', type=str, default='test',
                       choices=['train', 'valid', 'test'],
                       help='Dataset split to validate on (default: test)')
    parser.add_argument('--imgsz', type=int, default=768,
                       help='Input image size (default: 768)')
    parser.add_argument('--batch', type=int, default=16,
                       help='Batch size (default: 16)')
    
    args = parser.parse_args()
    
    # Check data.yaml exists
    data_yaml = Path(args.data)
    if not data_yaml.exists():
        print(f"âŒ Error: data.yaml not found at {data_yaml}")
        return 1
    
    # Expand wildcards and collect model paths
    model_paths = []
    for pattern in args.models:
        matches = glob.glob(pattern)
        if matches:
            model_paths.extend([Path(p) for p in matches if Path(p).exists()])
        else:
            # Try as direct path
            p = Path(pattern)
            if p.exists():
                model_paths.append(p)
    
    if len(model_paths) < 2:
        print(f"âŒ Error: Need at least 2 models to compare")
        print(f"   Found: {len(model_paths)} model(s)")
        return 1
    
    # Remove duplicates
    model_paths = list(set(model_paths))
    
    # Print info
    print("="*80)
    print("MODEL COMPARISON STARTING")
    print("="*80)
    print(f"Models to compare: {len(model_paths)}")
    print(f"Dataset: {data_yaml}")
    print(f"Split:   {args.split}")
    print("="*80)
    print("")
    
    # Validate each model
    all_metrics = []
    for i, model_path in enumerate(model_paths, 1):
        print(f"[{i}/{len(model_paths)}] Validating: {model_path.parent.parent.name}")
        
        metrics = validate_single_model(
            model_path=model_path,
            data_yaml=data_yaml,
            split=args.split,
            imgsz=args.imgsz,
            batch=args.batch
        )
        
        if metrics:
            all_metrics.append(metrics)
            print(f"    âœ“ Avg Accuracy: {metrics['avg_acc']*100:.2f}%")
        else:
            print(f"    âœ— Failed")
        print("")
    
    # Print comparison
    print_comparison_table(all_metrics)
    
    return 0


if __name__ == '__main__':
    exit(main())