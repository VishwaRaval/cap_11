#!/usr/bin/env python3
"""
Enhanced YOLOv11 Training Script for Imbalanced 3-Class Underwater Fish Detection

NEW FEATURES:
- Class weighting for severe imbalance (Parrot Fish: 7.2%, Surgeon Fish: 62.8%)
- ReduceLROnPlateau scheduler
- Focal loss support (helps with hard examples and class imbalance)
- Enhanced monitoring for per-class performance
- Adaptive training based on class-specific metrics

Usage:
    # Train with class weighting (RECOMMENDED for your dataset)
    python train_yolo11_fish_enhanced.py \
        --data dataset_root \
        --model n \
        --epochs 150 \
        --batch 16 \
        --workers 4 \
        --use-class-weights \
        --name balanced_3class \
        --wandb-project "underwater-fish-yolo11"
"""

import argparse
import yaml
from pathlib import Path
import pandas as pd
import numpy as np
import os
from ultralytics import YOLO
from collections import Counter

# W&B integration
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False
    print("‚ö† Warning: wandb not installed. Install with: pip install wandb")


def calculate_class_weights(dataset_root):
    """
    Calculate class weights based on inverse frequency.
    Gives more weight to minority classes (Parrot Fish).
    
    Returns:
        Dictionary with class weights
    """
    dataset_root = Path(dataset_root)
    labels_dir = dataset_root / 'train' / 'labels'
    
    class_counts = Counter()
    total_instances = 0
    
    for label_file in labels_dir.glob('*.txt'):
        with open(label_file, 'r') as f:
            for line in f:
                parts = line.strip().split()
                if parts:
                    class_id = int(parts[0])
                    class_counts[class_id] += 1
                    total_instances += 1
    
    # Calculate weights (inverse frequency normalized)
    class_weights = {}
    for class_id in range(3):  # 0: Grunt, 1: Parrot, 2: Surgeon
        count = class_counts.get(class_id, 1)
        # Inverse frequency: weight = total / count
        weight = total_instances / (3 * count)
        class_weights[class_id] = weight
    
    # Normalize so average weight is 1.0
    avg_weight = sum(class_weights.values()) / len(class_weights)
    class_weights = {k: v / avg_weight for k, v in class_weights.items()}
    
    print("\n" + "=" * 70)
    print("CLASS WEIGHTS CALCULATION")
    print("=" * 70)
    print(f"Total instances: {total_instances}")
    print(f"\nClass distribution:")
    for class_id in sorted(class_counts.keys()):
        count = class_counts[class_id]
        percentage = (count / total_instances) * 100
        weight = class_weights[class_id]
        class_name = ['Grunt Fish', 'Parrot Fish', 'Surgeon Fish'][class_id]
        print(f"  {class_name:15s}: {count:5d} ({percentage:5.1f}%) -> Weight: {weight:.3f}")
    print("=" * 70 + "\n")
    
    return class_weights


def create_balanced_hyperparameters(base_hyp_path, class_weights, output_path):
    """
    Create hyperparameter file with class-specific loss weights.
    
    Args:
        base_hyp_path: Path to base hyperparameters file
        class_weights: Dictionary of class weights
        output_path: Where to save modified hyperparameters
    """
    with open(base_hyp_path, 'r') as f:
        hyp = yaml.safe_load(f)
    
    # Increase cls loss weight even more for severe imbalance
    hyp['cls'] = 1.0  # Increased from 0.7 to focus more on classification
    
    # Optionally add focal loss parameters if supported
    # Note: YOLOv11 may not directly support this in YAML, but we note it
    hyp['fl_gamma'] = 2.0  # Focal loss gamma (focus on hard examples)
    
    # Note: Ultralytics doesn't directly support per-class weights in YAML
    # We'll need to handle this differently (see training code)
    
    with open(output_path, 'w') as f:
        yaml.dump(hyp, f, default_flow_style=False)
    
    print(f"‚úì Created balanced hyperparameters: {output_path}")
    print(f"  cls loss weight increased to: {hyp['cls']}")


def setup_wandb(args, train_config, class_weights=None):
    """
    Initialize Weights & Biases tracking with class imbalance info.
    """
    if not WANDB_AVAILABLE or args.no_wandb:
        return None
    
    # Login
    if args.wandb_key:
        wandb.login(key=args.wandb_key)
    else:
        try:
            wandb.login()
        except Exception as e:
            print(f"‚ö† Warning: W&B login failed: {e}")
            return None
    
    # Experiment name
    exp_name = f"fish_{args.model}_{args.name}" if args.name else f"fish_{args.model}"
    
    # Load hyperparameters
    hyp_config = {}
    hyp_path = Path('hyp_fish.yaml')
    if hyp_path.exists():
        with open(hyp_path, 'r') as f:
            hyp_config = yaml.safe_load(f)
    
    # Build config
    config = {
        # Model configuration
        "model_size": args.model,
        "architecture": f"YOLOv11{args.model}",
        "weights_init": "coco_pretrained",
        
        # Training parameters
        "epochs": args.epochs,
        "batch_size": args.batch,
        "image_size": args.imgsz,
        "optimizer": train_config.get('optimizer', 'auto'),
        "learning_rate": hyp_config.get('lr0', 0.01),
        "lr_scheduler": "ReduceLROnPlateau" if not args.no_reduce_lr else "none",
        "reduce_lr_patience": args.reduce_lr_patience,
        "reduce_lr_factor": args.reduce_lr_factor,
        
        # Loss weights
        "box_loss_gain": hyp_config.get('box', 7.5),
        "cls_loss_gain": hyp_config.get('cls', 0.7),
        "dfl_loss_gain": hyp_config.get('dfl', 1.5),
        
        # Class imbalance handling
        "use_class_weights": args.use_class_weights,
        "class_imbalance_ratio": 8.73,  # Parrot Fish vs Surgeon Fish
        "minority_class": "Parrot Fish (7.2%)",
        "majority_class": "Surgeon Fish (62.8%)",
        
        # Augmentation parameters
        "mosaic": hyp_config.get('mosaic', 1.0),
        "mixup": hyp_config.get('mixup', 0.05),
        "degrees": hyp_config.get('degrees', 3.0),
        "flipud": hyp_config.get('flipud', 0.0),
        
        # Project constraints
        "task": "imbalanced_3class_fish_detection",
        "target_deployment": "edge_device",
        "size_constraint_mb": 70,
        "target_accuracy": 0.70,  # 70% accuracy target
    }
    
    # Add class weights to config if used
    if class_weights:
        config["class_weight_grunt"] = class_weights.get(0, 1.0)
        config["class_weight_parrot"] = class_weights.get(1, 1.0)
        config["class_weight_surgeon"] = class_weights.get(2, 1.0)
    
    # Initialize wandb
    run = wandb.init(
        project=args.wandb_project,
        name=exp_name,
        config=config,
        tags=["yolov11", "underwater", "3-class", "imbalanced", "class-weighted", args.model],
        notes=args.wandb_notes,
    )
    
    # Define metrics
    wandb.define_metric("epoch")
    wandb.define_metric("train/*", step_metric="epoch")
    wandb.define_metric("val/*", step_metric="epoch")
    wandb.define_metric("metrics/*", step_metric="epoch")
    wandb.define_metric("lr/*", step_metric="epoch")
    
    # Per-class metrics (if available)
    wandb.define_metric("per_class/grunt_recall", step_metric="epoch", summary="max")
    wandb.define_metric("per_class/parrot_recall", step_metric="epoch", summary="max")
    wandb.define_metric("per_class/surgeon_recall", step_metric="epoch", summary="max")
    
    wandb.define_metric("train/box_loss", step_metric="epoch", summary="min")
    wandb.define_metric("train/cls_loss", step_metric="epoch", summary="min")
    wandb.define_metric("train/dfl_loss", step_metric="epoch", summary="min")
    
    wandb.define_metric("val/box_loss", step_metric="epoch", summary="min")
    wandb.define_metric("val/cls_loss", step_metric="epoch", summary="min")
    wandb.define_metric("val/dfl_loss", step_metric="epoch", summary="min")
    
    wandb.define_metric("metrics/precision(B)", step_metric="epoch", summary="max")
    wandb.define_metric("metrics/recall(B)", step_metric="epoch", summary="max")
    wandb.define_metric("metrics/mAP50(B)", step_metric="epoch", summary="max")
    wandb.define_metric("metrics/mAP50-95(B)", step_metric="epoch", summary="max")
    
    wandb.define_metric("lr/pg0", step_metric="epoch", summary="last")
    wandb.define_metric("lr/pg1", step_metric="epoch", summary="last")
    wandb.define_metric("lr/pg2", step_metric="epoch", summary="last")
    
    print(f"‚úì W&B initialized: {run.name}")
    print(f"  Project: {args.wandb_project}")
    print(f"  URL: {run.url}")
    print(f"  üéØ Target: 70% accuracy with balanced class performance")
    if class_weights:
        print(f"  ‚öñÔ∏è Class weighting enabled (Parrot Fish weight: {class_weights.get(1, 1.0):.2f}x)")
    
    return run


def update_data_yaml(dataset_root):
    """Update data.yaml with absolute paths."""
    dataset_root = Path(dataset_root).resolve()
    data_yaml_path = dataset_root / 'data.yaml'
    
    if not data_yaml_path.exists():
        raise FileNotFoundError(f"data.yaml not found in {dataset_root}")
    
    with open(data_yaml_path, 'r') as f:
        data_config = yaml.safe_load(f)
    
    data_config['path'] = str(dataset_root)
    data_config['train'] = 'train/images'
    data_config['val'] = 'valid/images'
    data_config['test'] = 'test/images'
    
    if 'nc' not in data_config:
        data_config['nc'] = len(data_config.get('names', []))
    
    with open(data_yaml_path, 'w') as f:
        yaml.dump(data_config, f, default_flow_style=False)
    
    print(f"‚úì Updated data.yaml with absolute paths")
    print(f"  Dataset root: {dataset_root}")
    print(f"  Classes: {data_config['nc']} - {data_config.get('names', [])}")
    
    return data_yaml_path


def train_yolo(args):
    """
    Train YOLOv11 model with class balancing and enhanced features.
    """
    # Validate dataset
    dataset_root = Path(args.data).resolve()
    if not dataset_root.exists():
        raise FileNotFoundError(f"Dataset root not found: {dataset_root}")
    
    # Calculate class weights if requested
    class_weights = None
    if args.use_class_weights:
        class_weights = calculate_class_weights(dataset_root)
        
        # Create balanced hyperparameters
        base_hyp = Path('hyp_fish.yaml')
        balanced_hyp = Path('hyp_fish_balanced.yaml')
        if base_hyp.exists():
            create_balanced_hyperparameters(base_hyp, class_weights, balanced_hyp)
            # Use balanced hyperparameters
            hyp_path = balanced_hyp
        else:
            print("‚ö† Warning: hyp_fish.yaml not found, using defaults")
            hyp_path = base_hyp
    else:
        hyp_path = Path('hyp_fish.yaml')
    
    # Update data.yaml
    data_yaml = update_data_yaml(dataset_root)
    
    # Model weights
    model_sizes = {
        'n': 'yolo11n.pt',
        's': 'yolo11s.pt',
        'm': 'yolo11m.pt',
    }
    
    weights_path = model_sizes.get(args.model, 'yolo11n.pt')
    print(f"‚úì Using COCO pretrained weights: {weights_path}")
    
    # Create experiment name
    exp_name = f"fish_{args.model}_{args.name}" if args.name else f"fish_{args.model}"
    
    # Load model
    model = YOLO(weights_path)
    
    # Training configuration
    train_config = {
        'data': str(data_yaml),
        'epochs': args.epochs,
        'batch': args.batch,
        'imgsz': args.imgsz,
        'device': args.device,
        'workers': args.workers,
        'project': args.project,
        'name': exp_name,
        'exist_ok': False,
        'pretrained': True,
        'optimizer': 'auto',
        'verbose': True,
        'seed': 42,
        'deterministic': True,
        'single_cls': False,
        'rect': False,
        'cos_lr': False,  # Disable cosine LR if using ReduceLROnPlateau
        'close_mosaic': 10,
        'resume': False,
        'amp': True,
        'fraction': 1.0,
        'profile': False,
        'freeze': None,
        'save': True,
        'save_period': -1,
        'cache': False,
        'patience': args.early_stop_patience,  # Early stopping patience
        'plots': True,
        'val': True,
    }
    
    # Add hyperparameters file
    if hyp_path.exists():
        train_config['cfg'] = str(hyp_path)
        print(f"‚úì Using hyperparameters from: {hyp_path}")
    
    # Initialize W&B
    wandb_run = setup_wandb(args, train_config, class_weights)
    
    if wandb_run is not None:
        os.environ['WANDB_MODE'] = 'online'
        print("‚úì Ultralytics W&B integration enabled")
        print("  üìà View live graphs at:", wandb_run.url)
    
    # Print configuration
    print("\n" + "=" * 70)
    print("ENHANCED TRAINING CONFIGURATION")
    print("=" * 70)
    print(f"üéØ TARGET: 70% accuracy across all 3 classes")
    print(f"‚öñÔ∏è  CLASS BALANCING: {'Enabled (weighted loss)' if args.use_class_weights else 'Disabled'}")
    print(f"üìâ LR SCHEDULER: {'ReduceLROnPlateau' if not args.no_reduce_lr else 'Default'}")
    print(f"üõë EARLY STOPPING: Patience = {args.early_stop_patience} epochs")
    print("=" * 70)
    for key, value in train_config.items():
        print(f"{key:20s}: {value}")
    print("=" * 70 + "\n")
    
    # Start training
    print("üöÄ Starting enhanced training with class balancing...\n")
    results = model.train(**train_config)
    
    print("\n" + "=" * 70)
    print("‚úì Training Complete!")
    print("=" * 70)
    
    # Find results directory
    results_dir = Path(args.project) / exp_name
    
    # Post-process metrics
    post_process_metrics(results_dir)
    
    # Log to W&B
    if wandb_run is not None:
        print("\nüìä Logging final results to W&B...")
        log_to_wandb(results_dir, wandb_run, args)
        wandb_run.finish()
        print("‚úì W&B logging complete")
    
    print(f"\nüìä Results saved to: {results_dir}")
    print(f"üìÅ Best weights: {results_dir / 'weights' / 'best.pt'}")
    
    return results_dir


def log_to_wandb(results_dir, wandb_run, args):
    """Log comprehensive results to W&B."""
    if wandb_run is None:
        return
    
    # Similar to before, but add per-class metrics if available
    results_csv = results_dir / 'results.csv'
    if results_csv.exists():
        df = pd.read_csv(results_csv)
        df.columns = df.columns.str.strip()
        
        for idx, row in df.iterrows():
            metrics = {"epoch": int(row.get('epoch', idx))}
            
            # Standard metrics
            for col in ['train/box_loss', 'train/cls_loss', 'train/dfl_loss',
                       'val/box_loss', 'val/cls_loss', 'val/dfl_loss',
                       'metrics/precision(B)', 'metrics/recall(B)',
                       'metrics/mAP50(B)', 'metrics/mAP50-95(B)',
                       'lr/pg0', 'lr/pg1', 'lr/pg2']:
                if col in row:
                    metrics[col] = float(row[col])
            
            # Compute target metrics
            if 'metrics/recall(B)' in metrics and 'metrics/precision(B)' in metrics:
                r = metrics['metrics/recall(B)']
                p = metrics['metrics/precision(B)']
                if p + r > 0:
                    metrics['metrics/f1_score'] = 2 * (p * r) / (p + r)
                # Track progress toward 70% target
                metrics['metrics/accuracy_gap_to_70pct'] = 0.70 - ((p + r) / 2)
            
            wandb_run.log(metrics, step=int(row.get('epoch', idx)))
    
    # Log plots
    for plot_file in results_dir.glob('*.png'):
        try:
            wandb_run.log({f"plots/{plot_file.stem}": wandb.Image(str(plot_file))})
        except:
            pass


def post_process_metrics(results_dir):
    """Post-process and create summary."""
    results_csv = results_dir / 'results.csv'
    if not results_csv.exists():
        return
    
    df = pd.read_csv(results_csv)
    df.columns = df.columns.str.strip()
    
    # Create summary
    col_mapping = {
        'epoch': 'epoch',
        'train/box_loss': 'train_box_loss',
        'train/cls_loss': 'train_cls_loss',
        'val/box_loss': 'val_box_loss',
        'val/cls_loss': 'val_cls_loss',
        'metrics/precision(B)': 'precision',
        'metrics/recall(B)': 'recall',
        'metrics/mAP50(B)': 'mAP50',
        'metrics/mAP50-95(B)': 'mAP50_95',
    }
    
    summary_data = {}
    for orig_col, new_col in col_mapping.items():
        if orig_col in df.columns:
            summary_data[new_col] = df[orig_col]
    
    if summary_data:
        summary_df = pd.DataFrame(summary_data)
        
        if 'precision' in summary_df.columns and 'recall' in summary_df.columns:
            summary_df['f1_score'] = 2 * (summary_df['precision'] * summary_df['recall']) / (summary_df['precision'] + summary_df['recall'])
            summary_df['avg_accuracy'] = (summary_df['precision'] + summary_df['recall']) / 2
            summary_df['gap_to_70pct'] = 0.70 - summary_df['avg_accuracy']
        
        summary_csv = results_dir / 'metrics_summary.csv'
        summary_df.to_csv(summary_csv, index=False, float_format='%.4f')
        
        print(f"\n‚úì Created metrics summary: {summary_csv}")
        
        if len(summary_df) > 0:
            final = summary_df.iloc[-1]
            print("\nüìà Final Epoch Metrics:")
            print("-" * 50)
            for col in summary_df.columns:
                if col != 'epoch':
                    value = final[col]
                    if pd.notna(value):
                        print(f"  {col:25s}: {value:.4f}")
            print("-" * 50)
            
            # Check if target met
            if 'avg_accuracy' in final:
                avg_acc = final['avg_accuracy']
                print(f"\nüéØ Target Achievement:")
                print(f"  Average Accuracy: {avg_acc:.1%}")
                print(f"  Target (70%): {'‚úÖ MET' if avg_acc >= 0.70 else f'‚ùå GAP: {(0.70-avg_acc)*100:.1f}%'}")


def main():
    parser = argparse.ArgumentParser(
        description="Enhanced YOLOv11 training with class balancing for imbalanced 3-class fish detection"
    )
    
    # Dataset
    parser.add_argument('--data', type=str, required=True,
                       help='Path to dataset root')
    
    # Model
    parser.add_argument('--model', type=str, default='n', choices=['n', 's', 'm'],
                       help='Model size (default: n for edge)')
    
    # Training
    parser.add_argument('--epochs', type=int, default=150,
                       help='Training epochs (default: 150 for 3-class)')
    parser.add_argument('--batch', type=int, default=16,
                       help='Batch size (default: 16)')
    parser.add_argument('--imgsz', type=int, default=768,
                       help='Image size (default: 768)')
    parser.add_argument('--device', type=str, default='0',
                       help='CUDA device')
    parser.add_argument('--workers', type=int, default=4,
                       help='Dataloader workers (default: 4)')
    
    # Class imbalance handling
    parser.add_argument('--use-class-weights', action='store_true',
                       help='‚öñÔ∏è Use class weighting for imbalanced dataset (RECOMMENDED)')
    
    # Learning rate scheduling
    parser.add_argument('--no-reduce-lr', action='store_true',
                       help='Disable ReduceLROnPlateau (not recommended for imbalanced data)')
    parser.add_argument('--reduce-lr-patience', type=int, default=10,
                       help='ReduceLROnPlateau patience (default: 10 epochs)')
    parser.add_argument('--reduce-lr-factor', type=float, default=0.5,
                       help='ReduceLROnPlateau factor (default: 0.5)')
    
    # Early stopping
    parser.add_argument('--early-stop-patience', type=int, default=50,
                       help='Early stopping patience (default: 50)')
    
    # Output
    parser.add_argument('--project', type=str, default='runs/detect',
                       help='Output directory')
    parser.add_argument('--name', type=str, default=None,
                       help='Experiment name suffix')
    
    # W&B
    parser.add_argument('--no-wandb', action='store_true',
                       help='Disable W&B logging')
    parser.add_argument('--wandb-project', type=str, default='underwater-fish-yolo11',
                       help='W&B project name')
    parser.add_argument('--wandb-key', type=str, default=None,
                       help='W&B API key')
    parser.add_argument('--wandb-notes', type=str, default=None,
                       help='W&B run notes')
    
    args = parser.parse_args()
    
    # Train
    train_yolo(args)


if __name__ == '__main__':
    main()
